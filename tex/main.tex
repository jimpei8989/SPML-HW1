\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
     \usepackage[final, nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{multirow}
\usepackage{makecell}

\title{Security and Privacy of Machine Learning - Homework 1}

\author{
  Wu-Jun Pei\\
  National Taiwan University\\
  \texttt{b06902029@ntu.edu.tw} \\
}

\begin{document}

\maketitle

\begin{abstract}
  In this homework, I implemented adversarial attacks against CIFAR-10 image classifiers under a gray-box setting. I tried several attacking methods and consider multiple attacking parameters. Furthermore, I also implemented some possible preprocessing-based defenses so that I can evaluate in a more approporiate setting. Based on the experiments I made, I finally selected the final adversarial examples that could tear down xxx\% on our evaluation set.
\end{abstract}

\section{Introduction}
Powerful as neural networks are, they are also vulnerable to malicious adversarial examples. The perturbations are imperceptible to human; however, they are likely to make the model output the wrong answer with high confidence.

In this homework, I implemented adversarial attacks against CIFAR-10 image classifiers under a gray-box
setting. The models are chosen from imgcls\cite{imgcls}, and the adversarial perturbation is $l_{\infty} = 8 / 256$.

Code is available at: \url{https://github.com/jimpei8989/SPML-HW1}.

\section{Attacking Details}
In this section, I'll explain the attacking methods and attacking techniques I use. Moreover, some experiments are conducted to achieve the final adversarial examples. During the experiments, the attack accuracies are not the only metric I consider, transferbility is also taken into account since it's a gray-box setting. Intuitively, I use the standard deviation of the accuracies among different models to examine transferbility. The greater the value is, the less transferbility it is.

\subsection{Attacking Mechanisms}

To find adversarial examples, that is, to find a perturbation $\delta$ such that

\begin{equation}
  \delta = \mathop{\arg\min}_{x' \in \mathrm{Adv}(x)} (x', y; \theta)
\end{equation}

The simplest method is Fast Gradient Sign Method\cite{goodfellow2014explaining}. To achieve better attack performance, we also tried Iterative Fast Gradient Sign Method (I-FGSM)\cite{kurakin2016adversarial}, Momentum Iterative Fast Gradient Sign Method (MI-FGSM)\cite{dong2018boosting} and Projective Gradient Descent\cite{madry2017towards}.

The experiment results are shown on table \ref{attack-experiments}. In I-FGSM, MI-FGSM and PGD, the number of iterations is selected to be 4 based on some random experiments. The magic number selected on my own, considering both attack accuracy and transferbility.

From the table, we can tell that FGSM is simple and already performances well. On the other hand, the iterative methods, I-FGSM, MI-FGSM and PGD, lead to an ever better attack accuracy. However, they have little transferbility as the standard deviations are much larger. Also, we can observe that those non-proxy models have similar accuracies among row 1, row 5, row 9 and row 13. I think all these methods focus only on the proxy model so that the adversarial examples have limited effect on other models.

As for the selection of proxy model, it's relatively better when we choose ResNet20. Thus, I'll choose it as the experiment proxy model in the next sections. Also, I'll consider PGD as the major attack mechanism.

\begin{table}
  \label{attack-experiments}
  \centering
  \caption{Experiment results. The evaluation models A-F are \texttt{nin}, \texttt{resnet20}, \texttt{resnext29-32x4d}, \texttt{seresnet20}, \texttt{pyramidnet110-a48}, \texttt{densenet40-k12}, respectively. Bold texts indicate the proxy model is the same as the evaluation model.}
  \begin{tabular}{cccccccccc}
    \toprule
    \multicolumn{2}{c}{Settings} & \multicolumn{8}{c}{Evaluation models} \\
    \cmidrule(r){1-2} \cmidrule(r){3-10}
    \makecell{Mechanisms}     & \makecell{Proxy \\ Model} & A & B & C & D & E & F & Avg. & Std. Dev. \\
    \midrule

    None                  & None        & 0.94 & 0.96 & 1.00 & 0.97 & 1.00 & 0.98 & 0.9750 & 0.0214 \\
    \midrule

    \multirow{4}{*}{FGSM} & ResNet20    & 0.64 & \textbf{0.36} & 0.54 & 0.53 & 0.56 & 0.53 & 0.5267 & 0.0836 \\
                          & ResNext20   & 0.81 & 0.73 & \textbf{0.45} & 0.70 & 0.63 & 0.72 & 0.6733 & 0.1129 \\
                          & DenseNet40  & 0.73 & 0.67 & 0.58 & 0.71 & 0.65 & \textbf{0.56} & 0.6500 & 0.0624 \\
                          & Average     & \\
    \midrule

    \multirow{4}{*}{\shortstack{I-FGSM \\ \# iterations = 4}}
                          & ResNet20    & 0.77 & \textbf{0.03} & 0.74 & 0.41 & 0.50 & 0.42 & 0.4783 & 0.2460 \\
                          & ResNext20   & 0.90 & 0.80 & \textbf{0.06} & 0.77 & 0.58 & 0.67 & 0.6300 & 0.2740 \\
                          & DenseNet40  & 0.86 & 0.62 & 0.71 & 0.56 & 0.55 & \textbf{0.10} & 0.5667 & 0.2336 \\
                          & Average     & \\
    \midrule

    \multirow{4}{*}{\shortstack{MI-FGSM \\ \# iterations = 4}}
                          & ResNet20    & 0.67 & \textbf{0.03} & 0.69 & 0.38 & 0.44 & 0.40 & 0.4350 & 0.2193 \\
                          & ResNext20   & 0.89 & 0.79 & \textbf{0.04} & 0.74 & 0.58 & 0.66 & 0.6167 & 0.2756 \\
                          & DenseNet40  & 0.82 & 0.62 & 0.64 & 0.53 & 0.52 & \textbf{0.10} & 0.5383 & 0.2194 \\
                          & Average     & \\
    \midrule

    \multirow{4}{*}{\shortstack{PGD \\ \# iterations = 4}}
                          & ResNet20    & 0.63 & \textbf{0.00} & 0.55 & 0.31 & 0.41 & 0.31 & 0.3683 & 0.2024 \\
                          & ResNext20   & 0.86 & 0.73 & \textbf{0.03} & 0.66 & 0.43 & 0.55 & 0.5433 & 0.2662 \\
                          & DenseNet40  & 0.77 & 0.54 & 0.51 & 0.60 & 0.43 & \textbf{0.04} & 0.4817 & 0.2233 \\
                          & Average     & \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Proxy Model Selection}
Selection of proxy models plays an important role in black / gray box attacks. A better selection may lead to better transferbility and thus make the attack more successful without knowing the exact model. In this part, we want to discuss the selection in three manners, \textit{one weak model}, \textit{one strong model}, and \textit{ensemble of multiple models}.

First of all, let's discuss first two first. The \textit{weak} models and \textit{strong} models are mainly examined by the number of layers, parameters and FLOPs. In this experiment, I'll take \texttt{ResNet20} and \texttt{ResNet1001} as the \textit{weak} and \textit{strong} models, respectively.

\begin{table}
  \label{proxy-experiments}
  \centering
  \caption{Experiment results. The evaluation models A-G are \texttt{nin}, \texttt{resnet20}, \texttt{resnext29-32x4d}, \texttt{seresnet20}, \texttt{pyramidnet110-a48}, \texttt{densenet40-k12}, \texttt{resnet1001}, respectively. Bold texts indicate the proxy model is the same as the evaluation model.}
  \begin{tabular}{cccccccccc}
    \toprule
    \makecell{Proxy \\ Model} & A & B & C & D & E & F & G & Avg. & Std. Dev. \\

    \bottomrule
  \end{tabular}
\end{table}


\subsection{Possible Defenses}
To evaluate our attack more precisely, we add two possible preprocessing-based defenses in the evaluation phase.
\begin{itemize}
  \item \textbf{Gaussian Blur}: We tried to add some gaussian blur in the evaluation phase. However, even we set the radius to 1 have a terrible accuracy. Since the image size is 32x32, a 3x3 filter might be so big for it that the classifiers are not able to classify the blur images even they haven't been perturbed.
  \item \textbf{JPEG Compression} \cite{das2017keeping}: We can adopt the systematic compression method to reduce adversarial noise in the preprocessing phase. In this experiment, we tried two quality parameters, 60 and 80, which are both able to
\end{itemize}

\begin{table}
  \label{defense-experiments}
  \centering
  \caption{Experiment results. For the first four rows, they are tested on the benign examples with different defense methods.}
  \begin{tabular}{cccccccccc}
    \toprule
    \multicolumn{2}{c}{Settings} & \multicolumn{8}{c}{Evaluation models} \\
    \cmidrule(r){1-2} \cmidrule(r){3-10}
    \makecell{Attack \\ Method} & \makecell{Defense \\ Method} & A & B & C & D & E & F & Avg. & Std. Dev. \\
    \midrule

    \multirow{4}{*}{None} & None                & 0.94 & 0.96 & 1.00 & 0.97 & 1.00 & 0.98 & 0.9750 & 0.0214 \\
                          & Gaussian (radius=1) & 0.63 & 0.57 & 0.38 & 0.49 & 0.53 & 0.50 & 0.5167 & 0.0770 \\
                          & JPEG (quality=80)   & 0.86 & 0.83 & 0.92 & 0.85 & 0.85 & 0.81 & 0.8533 & 0.0340 \\
                          & JPEG (quality=60)   & 0.85 & 0.77 & 0.83 & 0.77 & 0.79 & 0.79 & 0.8000 & 0.0300 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsubsection*{Experiments}

\subsection{Some Miscellious Techiniques Used}

\subsection{Final Decision}

\section{Conclusion}

\bibliographystyle{ieeetr}
\bibliography{citation}

\end{document}
